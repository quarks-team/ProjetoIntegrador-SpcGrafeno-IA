# -*- coding: utf-8 -*-
"""AI_MODEL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XMAT60MJItnQ34SGyEmnsWe9QVVZPjZ4

# Prepare the base

#### Import data and python libraries
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder

base = pd.read_excel('/content/sample_data/table.xlsx', sheet_name = 'Planilha1')

"""#### Transform data"""

base_test = base

base_test['a'] = (base_test.iloc[:,8] - base_test.iloc[:,9]).dt.days
base_test = base_test[[base_test.iloc[:,18].name, base_test.iloc[:,4].name, base_test.iloc[:,5].name, base_test.iloc[:,19].name, base_test.iloc[:,16].name]]
base_test.iloc[:,4] =  base_test.iloc[:,4].fillna('not canceled')
base_test = pd.get_dummies(base_test, columns = [base_test.iloc[:,1].name],dtype=int)

# Separate the column for one hot
categorical_attributes = [base_test.iloc[:,1].name]
categorical_columns = base_test[categorical_attributes]
# instance and train OneHotEncoder
encoder = OneHotEncoder(handle_unknown='ignore')
encoder.fit(categorical_columns)
# Codify the categorical columns and transform in a dataframe
encoded = encoder.transform(categorical_columns).toarray()
enc_train = pd.DataFrame(data = encoded, columns = encoder.categories_)
# Concat with the original dataframe and drop original column
base_test = pd.concat([base_test,enc_train],axis=1)
base_test.drop(categorical_attributes, axis=1, inplace=True)

# Separate the column for one hot
categorical_attributes = [base_test.iloc[:,2].name]
categorical_columns = base_test[categorical_attributes]
# instance and train OneHotEncoder
encoder = OneHotEncoder(handle_unknown='ignore')
encoder.fit(categorical_columns)
# Codify the categorical columns and transform in a dataframe
encoded = encoder.transform(categorical_columns).toarray()
enc_train = pd.DataFrame(data = encoded, columns = encoder.categories_)
# Concat with the original dataframe and drop original column
base_test = pd.concat([base_test,enc_train],axis=1)
base_test.drop(categorical_attributes, axis=1, inplace=True)

# Rename columns
base_test.columns

base_test = base_test.rename(columns={ base_test.iloc[:,4].name: str(base_test.iloc[:,4].name)[2:-3], base_test.iloc[:,5].name: str(base_test.iloc[:,5].name)[2:-3],base_test.iloc[:,6].name: str(base_test.iloc[:,6].name)[2:-3], base_test.iloc[:,7].name: str(base_test.iloc[:,7].name)[2:-3], base_test.iloc[:,8].name: str(base_test.iloc[:,8].name)[2:-3], base_test.iloc[:,9].name: str(base_test.iloc[:,9].name)[2:-3], base_test.iloc[:,10].name: str(base_test.iloc[:,10].name)[2:-3]})

base_test.isna().sum()

function_dictionary = {base_test.iloc[:,1].name:'mean',base_test.iloc[:,2].name:'sum',base_test.iloc[:,3].name:'sum', base_test.iloc[:,4].name:'sum', base_test.iloc[:,5].name:'sum', base_test.iloc[:,6].name: 'sum', base_test.iloc[:,7].name: 'sum', base_test.iloc[:,8].name: 'sum',	base_test.iloc[:,9].name: 'sum',	base_test.iloc[:,10].name: 'sum'}
# base_crazy = base_test
base_test = base_test.groupby(base_test.iloc[:,0].name).aggregate(function_dictionary).reset_index(0)

base_nfe = base

base_nfe = base_nfe.groupby([base_nfe.iloc[:,18].name, base_nfe.iloc[:,2].name]).agg({base_nfe.iloc[:,0].name: pd.Series.nunique}).reset_index(0)

base_nfe = base_nfe.reset_index(0)

base_nfe = base_nfe.groupby(base_nfe.iloc[:,1].name).agg({base_nfe.iloc[:,2].name: 'median'}).reset_index(0)

base_test = base_test.set_index(base_nfe.iloc[:,0].name).join(base_nfe.set_index(base_nfe.iloc[:,0].name)).reset_index(0)

base_test = base_test.rename(columns={base_test.iloc[:,11].name: "installment median"})

base_test.fillna(base_test[base_test.iloc[:,11].name].median(), inplace=True)

base_test['t'] = base_test.iloc[:,4] + base_test.iloc[:,5] + base_test.iloc[:,6]

base_test['pnc'] = base_test.iloc[:,7] / base_test.iloc[:,12]

"""#### Create scoring"""

base_test['score_f'] = np.where(
    base_test.iloc[:,6] <= 5,(1000*base_test.iloc[:,6])/5,
    np.where(
        (base_test.iloc[:,6] > 5) & (base_test.iloc[:,6] <= 18),(3000*base_test.iloc[:,6])/18,
    np.where(
        (base_test.iloc[:,6] > 18) & (base_test.iloc[:,6] <= 30), (4999*base_test.iloc[:,6])/30,
        5000
    )
        )
    )

base_test['score_a'] = np.where(
    base_test.iloc[:,4] <= 40,(600*base_test.iloc[:,4])/40,
    np.where(
        (base_test.iloc[:,4] > 40) & (base_test.iloc[:,4] <= 199),(1800*base_test.iloc[:,4])/199,
    np.where(
        (base_test.iloc[:,4] > 199) & (base_test.iloc[:,4] <= 484), (2999*base_test.iloc[:,4])/484,
        3000
    )
        )
    )

base_test['score_t'] = np.where(
    base_test.iloc[:,12] <= 40,(400*base_test.iloc[:,12])/40,
    np.where(
        (base_test.iloc[:,12] > 40) & (base_test.iloc[:,12] <= 100),(1200*base_test.iloc[:,12])/100,
    np.where(
        (base_test.iloc[:,12] > 100) & (base_test.iloc[:,12] <= 200), (1999*base_test.iloc[:,12])/200,
        2000
    )
        )
    )

base_test['score_c'] = np.where(
    base_test.iloc[:,5] <= 13,(200*base_test.iloc[:,5])/13,
    np.where(
        (base_test.iloc[:,5] > 13) & (base_test.iloc[:,5] <= 35),(600*base_test.iloc[:,5])/35,
    np.where(
        (base_test.iloc[:,5] > 35) & (base_test.iloc[:,5] <= 65), (999*base_test.iloc[:,5])/65,
        1000
    )
        )
    )

base_test.iloc[:,17]

base_test['s'] = base_test.iloc[:,14] + base_test.iloc[:,15] + base_test.iloc[:,16] - base_test.iloc[:,17]

"""# IA - MODEL Nº1

"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_score # Utilizado para separar dados de treino e teste
from sklearn.preprocessing import StandardScaler # Utilizado para fazer a normalização dos dados
from sklearn.preprocessing import MinMaxScaler # Utilizado para fazer a normalização dos dados
from sklearn.preprocessing import LabelEncoder # Utilizado para fazer o OneHotEncoding
from sklearn.linear_model import LinearRegression # Algoritmo de Regressão Linear
from sklearn.metrics import r2_score, accuracy_score,confusion_matrix, ConfusionMatrixDisplay # Métricas de avaliação do models
from sklearn.tree import DecisionTreeClassifier
import graphviz
from sklearn import tree

# Separate data for use
base_model = base_test[[base_test.iloc[:,1].name,	base_test.iloc[:,2].name,	base_test.iloc[:,3].name,base_test.iloc[:,4].name,	base_test.iloc[:,5].name,	base_test.iloc[:,6].name,	base_test.iloc[:,7].name,	base_test.iloc[:,11].name,	base_test.iloc[:,12].name,base_test.iloc[:,18].name]]
base_model = base_model[base_test.iloc[:,8] <= 1000]

base_model.describe()

X = base_model.loc[ : , base_model.columns != base_model.iloc[:,9].name]
y = base_model.iloc[:,9]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Normalization
sc = MinMaxScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

X_train

# Treina o modelo
model = LinearRegression()
model = model.fit(X_train, y_train)

# Accuracy
r2_score(y_test, model.fit(X_train, y_train).predict(X_test))

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# Previsões
y_pred = model.predict(X_test)

# Cálculo das métricas
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Exibição dos resultados
print(f'MAE: {mae}')
print(f'MSE: {mse}')
print(f'RMSE: {rmse}')
print(f'R²: {r2}')

# Try to predict score
v1 = 0
v2 = 520
v3 = 19
v4 = 471
v5 = 0
v6 = 68
v7 = 539
v8 = 0
v9 = 539

new_test = [v1,v2,v3,v4,v5,v6,v7,v8,v9]


X = np.array(new_test).reshape(1,-1)
X = sc.transform(X)
print("Score do endossante:", model.predict(X))

pesos = model.coef_
intercepto = model.intercept_
print( pesos)
print("Intercepto:", intercepto)